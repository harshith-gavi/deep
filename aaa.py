# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ImpVvINT-3YbrIVvTxs1YYZR4eViu84_
"""

pip install transformers
pip install datasets

import torch
from torch import nn
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, recall_score
from transformers import Trainer, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments

from scipy.special import softmax
from sklearn.model_selection import train_test_split
from datasets import load_metric
import csv



import urllib.request

from IPython.display import clear_output
clear_output()

def preprocess(text):
    new_text = []
 
 
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)

def compute_metrics(p):    
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    recall = recall_score(y_true=labels, y_pred=pred)
    f1 = f1_score(y_true=labels, y_pred=pred)
    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1} 

class Dataset(torch.utils.data.Dataset):    
    def __init__(self, encodings, labels=None):          
        self.encodings = encodings        
        self.labels = labels
     
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if self.labels:
            item["labels"] = torch.tensor(self.labels[idx])
        return item
    def __len__(self):
        return len(self.encodings["input_ids"])

from transformers import RobertaTokenizer
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

train_data = pd.read_csv('processed_data.csv')
test_data = pd.read_csv('test_data.csv', names=['polarity', 'id', 'date', 'query', 'user', 'text'])
test_data = test_data.drop(columns=['id', 'date', 'query', 'user'])

for i in range(len(train_data)):
    train_data['text'].iloc[i] = tokenizer(preprocess(train_data['text'].iloc[i]), padding=True, truncation=True, max_length=512)['input_ids']

for i in range(len(test_data)):
    test_data['text'].iloc[i] = tokenizer(preprocess(test_data['text'].iloc[i]), padding=True, truncation=True, max_length=512)['input_ids']

X = train_data['text']
X = pd.DataFrame(X.to_list())
Y = train_data[['polarity']]
Y['polarity'] = Y['polarity'].apply(lambda x: 1 if x == 4 else 0 if x == 0 else 2)
clear_output()

from transformers import RobertaTokenizer
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

test_data = pd.read_csv('test_data.csv', names=['polarity', 'id', 'date', 'query', 'user', 'text'])
test_data = test_data.drop(columns=['id', 'date', 'query', 'user'])

for i in range(len(test_data)):
    test_data['text'].iloc[i] = tokenizer(preprocess(test_data['text'].iloc[i]), padding=True, truncation=True, max_length=512)['input_ids']

X = test_data[['text']]
Y = test_data[['polarity']]
Y['polarity'] = Y['polarity'].apply(lambda x: 1 if x == 4 else 0 if x == 0 else 2)

train_dataset = Dataset(X, Y)

model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2)
training_args = TrainingArguments(
    output_dir="./fine_tune_bert_output",
    evaluation_strategy="steps",
    learning_rate=6e-4,
    per_device_train_batch_size=768,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps = 1000,
    run_name = "ep_10_tokenized_11",
    save_strategy='no'
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    compute_metrics=compute_metrics)
 
# Train pre-trained model
trainer.train()

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.initializers import random_uniform
from tensorflow.keras.optimizers import Adam

k_i = random_uniform(minval = -1, maxval = 1)

model = tf.keras.Sequential()
model.add(Dense(768, input_dim = 534, activation = 'gelu', kernel_initializer = k_i, bias_initializer='zeros'))
model.add(Dense(768, activation = 'gelu', kernel_initializer = k_i, bias_initializer='zeros'))
model.add(Dense(768, activation = 'gelu', kernel_initializer = k_i, bias_initializer='zeros'))
model.add(Dense(768, activation = 'gelu', kernel_initializer = k_i, bias_initializer='zeros'))
model.add(Dense(768, activation = 'gelu', kernel_initializer = k_i, bias_initializer='zeros'))
model.add(Dense(768, activation = 'gelu', kernel_initializer = k_i, bias_initializer='zeros'))
model.add(Dense(768, activation = 'gelu', kernel_initializer = k_i, bias_initializer='zeros'))
model.add(Dense(1, activation = 'softmax', kernel_initializer = k_i, bias_initializer='zeros'))

model.compile(optimizer = Adam(learning_rate=6e-4, beta_1=0.9, beta_2=0.98, epsilon=1e-06), loss = 'mean_squared_error', metrics = ['accuracy'])

history = model.fit(X, Y, epochs = 1000)

X

